{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: statsmodels is not installed. You will be unable to use the Naive Bayes Decoder\n",
      "\n",
      "WARNING: Xgboost package is not installed. You will be unable to use the xgboost decoder\n",
      "\n",
      "WARNING: Keras package is not installed. You will be unable to use all neural net decoders\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import iisignature as sig\n",
    "import sigkernel\n",
    "import torch\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,Normalizer,MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score,r2_score,mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,RandomForestRegressor,GradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegression,Lasso\n",
    "from sklearn.svm import SVC,SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from tslearn.svm import TimeSeriesSVC,TimeSeriesSVR\n",
    "from tslearn.neighbors import KNeighborsTimeSeriesClassifier\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from tslearn.neural_network import TimeSeriesMLPRegressor\n",
    "\n",
    "from Neural_Decoding.preprocessing_funcs import get_spikes_with_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_method_setup(method, X_train, y_train,reduced = False):\n",
    "        \"\"\"\n",
    "        This function is used to set up the machine learning method for the pipeline.\n",
    "\n",
    "        Parameters:\n",
    "                method: is the name of the method to be used, followings are the list of methods that can be used\n",
    "                        ['ts_knn','ts_svc','logisticregression','svc','knnclassifier','adaboostclassifier',\n",
    "                         'randomforestclassifier','r_ts_svr','r_ts_neuralnetwork''r_lassoregression','r_svr',\n",
    "                         'r_randomforestregression','r_gradientboostingregression','r_neuralnetwork','r_GaussianNB'ã€‘\n",
    "\n",
    "                X_train: is the training data\n",
    "                y_train: is the training label\n",
    "                reduced: is a boolean variable, if True, the hyperparameters will be reduced to save time\n",
    "\n",
    "\n",
    "        Returns:\n",
    "                clf: is the machine learning method with the best hyperparameters\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "        if method == 'ts_knn':\n",
    "                if reduced:\n",
    "                        parameters = {'n_neighbors': [1,5]}\n",
    "                else:\n",
    "                        parameters = {'n_neighbors': [1, 3, 5, 7],'metric': ['euclidean', 'dtw']}\n",
    "                clf = GridSearchCV(KNeighborsTimeSeriesClassifier(),\n",
    "                                parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'ts_svc':\n",
    "                if reduced:\n",
    "                        parameters = {'C': [0.1, 1, 10]}\n",
    "                else:\n",
    "                        parameters = {'C': [0.1, 1, 10],'kernel': ['linear', 'rbf'],'gamma': ['scale', 'auto', 0.1]}\n",
    "                clf = GridSearchCV(TimeSeriesSVC(random_state=0, probability=True),\n",
    "                                parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "        \n",
    "        elif method == 'r_ts_svr':\n",
    "                if reduced:\n",
    "                        parameters = {'C': [0.1, 1, 10]}\n",
    "                else:\n",
    "                        parameters = {'C': [0.1, 1, 10],'kernel': ['linear', 'rbf'],'gamma': ['scale', 'auto', 0.1]}\n",
    "                clf = GridSearchCV(TimeSeriesSVR(),\n",
    "                                parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'r_ts_neuralnetwork':\n",
    "                if reduced:\n",
    "                        parameters = {'hidden_layer_sizes': [(50,), (100,), (50, 50)]}\n",
    "                else:\n",
    "                        parameters = {'hidden_layer_sizes': [(50,), (100,), (50, 50)], \n",
    "                           'activation': ['relu', 'tanh'],  \n",
    "                           'learning_rate': ['constant', 'adaptive'],  \n",
    "                           'alpha': [0.0001, 0.001, 0.01]}\n",
    "                clf = GridSearchCV(TimeSeriesMLPRegressor(),\n",
    "                                parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'logisticregression':\n",
    "\n",
    "                lr = LogisticRegression(random_state=0)\n",
    "                if reduced:\n",
    "                        parameters = {'C': [0.1, 0.5, 1,  5]}\n",
    "                else:\n",
    "                        parameters = {'C': [0.1, 0.2, 0.5, 1, 2, 5, 10],\n",
    "                                      'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "                clf = GridSearchCV(lr, parameters,cv =5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'svc':\n",
    "                svc = SVC(random_state=0, probability=True)\n",
    "                if reduced:\n",
    "                        parameters = { 'C': [0.1, 1, 10]}\n",
    "                else:\n",
    "                        parameters = {'kernel': ['rbf', 'poly'], 'shrinking': [True, False],\n",
    "                                'C': [0.1, 1, 10]}\n",
    "                clf = GridSearchCV(svc, parameters,cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'knnclassifier':\n",
    "                knn = KNeighborsClassifier()\n",
    "                if reduced:\n",
    "                        parameters = {'n_neighbors': range(3, 30, 2)}\n",
    "                else:\n",
    "                        parameters = {'n_neighbors': range(3, 30, 2), 'weights': ['uniform', 'distance']}\n",
    "                clf = GridSearchCV(knn, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'adaboostclassifier':\n",
    "                ada = AdaBoostClassifier(random_state=0)\n",
    "                if reduced:\n",
    "                        parameters = {'n_estimators': [50, 100]}\n",
    "                else:\n",
    "                        parameters = {'n_estimators': [50, 100], 'learning_rate': [0.5, 1, 2]}\n",
    "                clf = GridSearchCV(ada, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'randomforestclassifier':\n",
    "                rf = RandomForestClassifier(random_state=0)\n",
    "                if reduced:\n",
    "                        parameters = {'n_estimators': (100, 200)}\n",
    "                else:\n",
    "                        parameters = {'min_weight_fraction_leaf': [0.1, 0.5],\n",
    "                                        'bootstrap': [True, False],\n",
    "                                        'max_depth': (2, 5),\n",
    "                                        'max_leaf_nodes': (2, 5),\n",
    "                                        'n_estimators': (100, 200)}\n",
    "                clf = GridSearchCV(rf, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'r_lassoregression':\n",
    "                lr = Lasso()\n",
    "                if reduced:\n",
    "                        parameters = {'alpha': [0.1, 0.5, 1, 5]}\n",
    "                else:\n",
    "                        parameters = {'alpha': [0.1, 0.2, 0.5, 1, 2, 5, 10]}\n",
    "                clf = GridSearchCV(lr, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "        \n",
    "        elif method == 'r_svr':\n",
    "                svr = SVR()\n",
    "                if reduced:\n",
    "                        parameters = {'C': [0.1, 1.0, 10.0]}\n",
    "                else:\n",
    "                        parameters = {'C': [0.1, 1.0, 10.0], 'kernel': ['linear', 'rbf'], \n",
    "                               'gamma': ['scale', 'auto']}\n",
    "                clf = GridSearchCV(svr, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'r_randomforestregression':\n",
    "                rf = RandomForestRegressor()\n",
    "                if reduced:\n",
    "                        parameters = {'n_estimators': [50, 100]}\n",
    "                else:\n",
    "                        parameters =  {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20], 'min_samples_split': [2, 5, 10]}\n",
    "                clf = GridSearchCV(rf, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'r_gradientboostingregression':\n",
    "                gb = GradientBoostingRegressor()\n",
    "                if reduced:\n",
    "                        parameters = {'n_estimators': [50, 100]}\n",
    "                else:\n",
    "                        parameters= {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}\n",
    "                clf = GridSearchCV(gb, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'r_neuralnetwork':\n",
    "                nn = MLPRegressor()\n",
    "                if reduced:\n",
    "                        parameters = {'hidden_layer_sizes': [(50,), (100,), (50, 50)]}\n",
    "                else:\n",
    "                        parameters = {'hidden_layer_sizes': [(50,), (100,), (50, 50)], 'activation': ['relu', 'tanh'], 'alpha': [0.0001, 0.001, 0.01]}\n",
    "                clf = GridSearchCV(nn, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "        \n",
    "        elif method == 'r_GaussianNB':\n",
    "                kernel = 1.0 * RBF(length_scale=1.0)\n",
    "                model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=42)\n",
    "                parameters = {'kernel__length_scale': [0.1, 1.0, 10.0],}        \n",
    "                clf = GridSearchCV(model, parameters, cv=5, n_jobs=-1,verbose=10,scoring='neg_mean_squared_error')\n",
    "                clf.fit(X_train, y_train)\n",
    "        else:\n",
    "                clf = None\n",
    "\n",
    "        return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_bins(X_train,y_train,X_test,y_test,bins_before,bins_after,bins_current):\n",
    "\n",
    "  \"\"\"\n",
    "  This function is used to create bins in order to decode the data.\n",
    "  See more details in https://www.eneuro.org/content/eneuro/7/4/ENEURO.0506-19.2020.full.pdf\n",
    "\n",
    "  Parameters:\n",
    "          X_train: is the training data with shape (num_timepoints, num_features)\n",
    "          y_train: is the training label with shape (num_timepoints,)\n",
    "          X_test: is the testing data with shape (num_timepoints, num_features)\n",
    "          y_test: is the testing label with shape (num_timepoints,)\n",
    "\n",
    "          bins_before: is the number of bins/timepoints before the current time point\n",
    "          bins_after: is the number of bins/timepoints after the current time point\n",
    "          bins_current: whether to include the current time point\n",
    "\n",
    "  Returns:\n",
    "          X_train: is the training data with shape (num_samples, num_bins(time), num_features)\n",
    "          y_train: is the training label with shape (num_samples,)\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  if bins_before == 0:\n",
    "    X_train =get_spikes_with_history(X_train,bins_before,bins_after,bins_current)[:-bins_after,:,:]\n",
    "    X_test = get_spikes_with_history(X_test,bins_before,bins_after,bins_current)[:-bins_after,:,:]\n",
    "    y_train = y_train[:-bins_after]\n",
    "    y_test = y_test[:-bins_after]\n",
    "\n",
    "  elif bins_after == 0:\n",
    "    X_train =get_spikes_with_history(X_train,bins_before,bins_after,bins_current)[bins_before:,:,:]\n",
    "    X_test = get_spikes_with_history(X_test,bins_before,bins_after,bins_current)[bins_before:,:,:]\n",
    "    y_train = y_train[bins_before:]\n",
    "    y_test = y_test[bins_before:]\n",
    "\n",
    "  else:\n",
    "    X_train =get_spikes_with_history(X_train,bins_before,bins_after,bins_current)[bins_before:-bins_after,:,:]\n",
    "    X_test = get_spikes_with_history(X_test,bins_before,bins_after,bins_current)[bins_before:-bins_after,:,:]\n",
    "    y_train = y_train[bins_before:-bins_after]\n",
    "    y_test = y_test[bins_before:-bins_after]\n",
    "\n",
    "  return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_flatten(X_train, X_test):\n",
    "    \"\"\"\n",
    "    This function is used to flatten the data.\n",
    "\n",
    "    Parameters:\n",
    "            X_train: is the training data with shape (num_samples, num_bins(time), num_features)\n",
    "            X_test: is the testing data with shape (num_samples, num_bins(time), num_features)\n",
    "\n",
    "    Returns:\n",
    "            X_train: is the training data with shape (num_samples, num_bins(time)*num_features)\n",
    "            X_test: is the testing data with shape (num_samples, num_bins(time)*num_features)\n",
    "    \"\"\"\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scale_process(X_train,X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function is used to standardize the data.\n",
    "\n",
    "    Parameters:\n",
    "            X_train: is the training data with shape (num_samples, num_bins(time)*num_features)\n",
    "            X_test: is the testing data with shape (num_samples, num_bins(time)*num_features)\n",
    "\n",
    "    Returns:\n",
    "            X_train: is the training data after standardization with shape (num_samples, num_bins(time)*num_features)\n",
    "            X_test: is the testing data after standardization with shape (num_samples, num_bins(time)*num_features)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    data_mean = np.nanmean(X_train,axis=0)\n",
    "    data_std = np.nanstd(X_train,axis=0)\n",
    "    if 0 in data_std: # if there is a feature with std = 0, then the data will be divided by 0, which will cause error\n",
    "      # report the error\n",
    "      print('There is a feature with std = 0, please check the data')\n",
    "    X_train = (X_train-data_mean)/data_std\n",
    "    X_test = (X_test-data_mean)/data_std\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(X_train, X_test, at, ll, scale = 'minmax'):\n",
    "\n",
    "    \"\"\"\n",
    "    This function is used to preprocess the data.\n",
    "\n",
    "    Parameters:\n",
    "            X_train: is the training data with shape (num_samples, num_bins(time), num_features)\n",
    "            X_test: is the testing data with shape (num_samples, num_bins(time), num_features)\n",
    "            at: whether add time reparemeterization\n",
    "            ll: whether add lead lag\n",
    "            scale: whether to scale the data, can be 'minmax' or 'standard'\n",
    "\n",
    "    Returns:\n",
    "            X_train: is the training data after preprocessing with shape (num_samples, num_bins(time)*num_features)\n",
    "            X_test: is the testing data after preprocessing with shape (num_samples, num_bins(time)*num_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    # ============ Data Preprocessing ============ \n",
    "    ts_min_max_scaler = TimeSeriesScalerMinMax()\n",
    "\n",
    "    # ============ minmax =============\n",
    "    if scale == 'minmax':\n",
    "        scaler = ts_min_max_scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    # ============ standard =============\n",
    "    elif scale == 'standard':\n",
    "        print(X_train.shape)\n",
    "        X_train, X_test = standard_scale_process(X_train, X_test)\n",
    "    \n",
    "    else: \n",
    "        pass\n",
    "    # ============ add time reparemeterization and lead lag =============\n",
    "    X_train = sigkernel.transform(X_train, at=at, ll=ll, scale=.1)\n",
    "    X_test = sigkernel.transform(X_test, at=at, ll=ll, scale=.1)\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signature(X_train,X_test,sig_level=2):\n",
    "    \"\"\"\n",
    "    This function is used to calculate the signature of the data.\n",
    "\n",
    "    Parameters:\n",
    "            X_train: is the training data with shape (num_samples, num_bins(time), num_features)\n",
    "            X_test: is the testing data with shape (num_samples, num_bins(time), num_features)\n",
    "            sig_level: is the level of the signature\n",
    "\n",
    "    Returns:\n",
    "            X_train: is the training data after signature with shape (num_samples, num_bins(time)*num_features)\n",
    "            X_test: is the testing data after signature with shape (num_samples, num_bins(time)*num_features)\n",
    "    \"\"\"\n",
    "    # hape of X_train: (n_samples,n_timestamps,n_features)\n",
    "    X_train = sig.sig(X_train,sig_level)\n",
    "    X_test = sig.sig(X_test,sig_level)\n",
    "\n",
    "    return X_train,X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(X_train,X_test):\n",
    "\n",
    "    \"\"\"\n",
    "    This function is used to subsample the data.\n",
    "\n",
    "    Parameters:\n",
    "            X_train: is the training data with shape (num_samples, num_bins(time), num_features)\n",
    "            X_test: is the testing data with shape (num_samples, num_bins(time), num_features)\n",
    "\n",
    "    Returns:\n",
    "            X_train: is the training data after subsampling with shape (num_samples, num_new_bins(time),num_features)\n",
    "            X_test: is the testing data after subsampling with shape (num_samples, num_new_bins(time),num_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    subsample = max(int(np.floor(X_train.shape[0]/149)),1)\n",
    "    X_train = X_train[:,::subsample,:]\n",
    "    X_test = X_test[:,::subsample,:]\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_atll(X_train,X_test,y_train,y_test,reduced = True):\n",
    "    \"\"\"\n",
    "    This function is used to find the best at,ll and scaler for the data.\n",
    "\n",
    "    Parameters:\n",
    "            X_train: is the training data with shape (num_samples, num_bins(time), num_features)\n",
    "            X_test: is the testing data with shape (num_samples, num_bins(time), num_features)\n",
    "            y_train: is the training label with shape (num_samples,)\n",
    "            y_test: is the testing label with shape (num_samples,)\n",
    "            reduced: is a boolean variable, if True, the hyperparameters will be reduced to save time\n",
    "\n",
    "    Returns:\n",
    "            best_at: is the best situation for whether add time reparemeterization\n",
    "            best_ll: is the best situation for whether add lead lag\n",
    "            best_scaler: is the best scaler\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if X_train.shape[1] <= 200 and X_train.shape[2] <= 8: \n",
    "                transforms = tqdm([(True,True), (False,True), (True,False), (False,False)], position=1, leave=False)\n",
    "    else: # do not try lead-lag as dimension is already high\n",
    "                transforms = tqdm([(True,False), (False,False)], position=1, leave=False)\n",
    "\n",
    "    best_score = 0\n",
    "    best_at = None\n",
    "    best_ll = None\n",
    "    best_scaler = None\n",
    "    scalers =  tqdm(['minmax', 'standard', None])\n",
    "    for at, ll in transforms:\n",
    "        transforms.set_description(f'at={at}, ll={ll}')\n",
    "        for scaler in scalers:\n",
    "            scalers.set_description(f'scaler={scaler}')\n",
    "            X_train_new, X_test_new = data_process(X_train, X_test, at, ll,scaler)\n",
    "            X_train_new, X_test_new = subsample(X_train_new,X_test_new)\n",
    "            clf = ml_method_setup('r_ts_svr', X_train_new,y_train,reduced)\n",
    "            if clf.best_score_ > best_score:\n",
    "                best_score = clf.best_score_\n",
    "                best_at = at\n",
    "                best_ll = ll\n",
    "                best_scaler = scaler\n",
    "\n",
    "    return best_at, best_ll, best_scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_bins(X_train,y_train,X_test,y_test,method,bin_reduce=True,reduced=True):\n",
    "\n",
    "    \"\"\"\n",
    "    This function is used to find the best bins_before for the data.\n",
    "\n",
    "    Parameters:\n",
    "            X_train: is the training data with shape (num_samples, num_bins(time), num_features)\n",
    "            X_test: is the testing data with shape (num_samples, num_bins(time), num_features)\n",
    "            y_train: is the training label with shape (num_samples,)\n",
    "            y_test: is the testing label with shape (num_samples,)\n",
    "            method: is the name of the method to be used, followings are the list of methods that can be use\n",
    "            bin_reduce: is a boolean variable, if True, the bins_before will be reduced to save time\n",
    "            reduced: is a boolean variable, if True, the hyperparameters will be reduced to save time\n",
    "    \n",
    "    Returns:\n",
    "            bin_b_best: is the best bins_before\n",
    "            \n",
    "    \"\"\"\n",
    "    if bin_reduce:\n",
    "        bins_before = tqdm([10,50,100,200])\n",
    "      \n",
    "    else:\n",
    "        bins_before = tqdm([5,10,25,50,75,100,250,500])\n",
    "\n",
    "    r2_best = 0\n",
    "    bin_b_best = 0\n",
    "   \n",
    "\n",
    "    for bin_before in bins_before:\n",
    "                bins_before.set_description(f'bin_before={bin_before}')\n",
    "                X_train_b,y_train_b,X_test_b,y_test_b = data_bins(X_train,y_train,X_test,y_test,bin_before,0,1)\n",
    "                X_train_b,X_test_b = subsample(X_train_b,X_test_b)\n",
    "                clf = ml_method_setup('r_ts_svr', X_train_b,y_train_b,reduced)\n",
    "                if clf.best_score_ > r2_best:\n",
    "                        bin_b_best = bin_before\n",
    "                        r2_best = clf.best_score_\n",
    "                   \n",
    "\n",
    "    return bin_b_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_signature(X_train,X_test,y_train,y_test,method,reduced):\n",
    "    \"\"\"\n",
    "    This function is used to find the best signature levelfor the data.\n",
    "\n",
    "    Parameters:\n",
    "            X_train: is the training data with shape (num_samples, num_bins(time), num_features)\n",
    "            X_test: is the testing data with shape (num_samples, num_bins(time), num_features)\n",
    "            y_train: is the training label with shape (num_samples,)\n",
    "            y_test: is the testing label with shape (num_samples,)\n",
    "            method: is the name of the method to be used, followings are the list of methods that can be use\n",
    "            reduced: is a boolean variable, if True, the hyperparameters will be reduced to save time\n",
    "\n",
    "    Returns:\n",
    "            best_depth: is the best signature level\n",
    "            best_scale: is the best scale to multiple the data\n",
    "\n",
    "    \"\"\"\n",
    "    dim  = X_train.shape[-1]\n",
    "   \n",
    "    if dim <= 4:\n",
    "        max_depth = 6\n",
    "    elif dim <= 6:\n",
    "        max_depth = 5\n",
    "    elif dim <= 8:\n",
    "        max_depth = 4\n",
    "    else:\n",
    "        max_depth = 3\n",
    "\n",
    "    _scales = [5e-2, 1e-1, 5e-1, 1e0]\n",
    "    scales = tqdm(_scales, position=3, leave=False)\n",
    "\n",
    "    # grid search on truncation levels\n",
    "    depths = tqdm(range(2,max_depth+1), position=2, leave=False)\n",
    "\n",
    "    best_score = 0\n",
    "    best_depth = None\n",
    "    best_scale = None\n",
    "\n",
    "    for depth in depths:\n",
    "        depths.set_description(f'depth={depth}')\n",
    "        for scale in scales:\n",
    "            scales.set_description(f'scale={scale}')\n",
    "            # truncated signatures\n",
    "            sig_train = sig.sig(scale*X_train, depth)\n",
    "            clf = ml_method_setup(method, sig_train,y_train,reduced)\n",
    "            if clf.best_score_ > best_score:\n",
    "                best_score = clf.best_score_\n",
    "                best_depth = depth\n",
    "                best_scale = scale\n",
    "                \n",
    "    return best_depth, best_scale\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signature_score(X_train,X_test,y_train,y_test,method,file_path,file_name,reduced = False):\n",
    "\n",
    "    \"\"\"\n",
    "    This function is used to calculate the score of the signature method and will save the score in a csv file.\n",
    "\n",
    "    Parameters:\n",
    "            X_train: is the training data with shape (num_samples, num_bins(time), num_features)\n",
    "            X_test: is the testing data with shape (num_samples, num_bins(time), num_features)\n",
    "            y_train: is the training label with shape (num_samples,)\n",
    "            y_test: is the testing label with shape (num_samples,)\n",
    "            method: is the name of the method to be used, followings are the list of methods that can be use\n",
    "            file_path: is the path to save the score\n",
    "            file_name: is the name of the file to save the score\n",
    "            reduced: is a boolean variable, if True, the hyperparameters will be reduced to save time\n",
    "\n",
    "    Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_depth, best_scale = grid_search_signature(X_train,X_test,y_train,y_test,method,reduced)\n",
    "    X_train_sig, X_test_sig = signature(X_train*best_scale, X_test*best_scale, best_depth)\n",
    "\n",
    "    clf_sig = ml_method_setup(method, X_train_sig, y_train, reduced)\n",
    "    y_pred_sig = clf_sig.predict(X_test_sig)\n",
    "    r2_sig = r2_score(y_test, y_pred_sig)\n",
    "    r_value_sig, p_value_sig = pearsonr(y_test, y_pred_sig)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_sig = end_time - start_time\n",
    "\n",
    "    score_sig = pd.DataFrame({'r2':r2_sig, 'rvalue':r_value_sig, 'time':time_sig}, index=[0])\n",
    "    score_sig.to_csv(f\"{file_path}{file_name}_sig.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_score(X_train,X_test,y_train,y_test,method,file_path,file_name,reduced = False):\n",
    "    \"\"\"\n",
    "    This function is used to calculate the score of the flatten method and will save the score in a csv file.\n",
    "    \n",
    "    Parameters:\n",
    "            X_train: is the training data with shape (num_samples, num_bins(time), num_features)\n",
    "            X_test: is the testing data with shape (num_samples, num_bins(time), num_features)\n",
    "            y_train: is the training label with shape (num_samples,)\n",
    "            y_test: is the testing label with shape (num_samples,)\n",
    "            method: is the name of the method to be used, followings are the list of methods that can be use\n",
    "            file_path: is the path to save the score\n",
    "            file_name: is the name of the file to save the score\n",
    "            reduced: is a boolean variable, if True, the hyperparameters will be reduced to save time\n",
    "\n",
    "    Returns:\n",
    "            None\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    X_train_flat, X_test_flat = data_flatten(X_train, X_test)\n",
    "    clf_flat = ml_method_setup(method, X_train_flat, y_train, reduced)\n",
    "    y_pred_flat = clf_flat.predict(X_test_flat)\n",
    "    r2_flat = r2_score(y_test, y_pred_flat)\n",
    "    r_value_flat, p_value_flat = pearsonr(y_test, y_pred_flat)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_flat = end_time - start_time\n",
    "\n",
    "    score_flat = pd.DataFrame({'r2':r2_flat, 'rvalue':r_value_flat, 'time':time_flat}, index=[0])\n",
    "    score_flat.to_csv(f\"{file_path}{file_name}_flat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_score(X_train,X_test,y_train,y_test,method,file_path,file_name,reduced = False):\n",
    "    \"\"\"\n",
    "\n",
    "    This function is used to calculate the score of the ts method and will save the score in a csv file.\n",
    "\n",
    "    Parameters:\n",
    "            X_train: is the training data with shape (num_samples, num_bins(time), num_features)\n",
    "            X_test: is the testing data with shape (num_samples, num_bins(time), num_features)\n",
    "            y_train: is the training label with shape (num_samples,)\n",
    "            y_test: is the testing label with shape (num_samples,)\n",
    "            method: is the name of the method to be used, followings are the list of methods that can be use\n",
    "            file_path: is the path to save the score\n",
    "            file_name: is the name of the file to save the score\n",
    "            reduced: is a boolean variable, if True, the hyperparameters will be reduced to save time\n",
    "\n",
    "    Returns:\n",
    "            None\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    clf = ml_method_setup(method, X_train, y_train, reduced)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    r2_ts = r2_score(y_test, y_pred)\n",
    "    r_value_ts, p_value_ts = pearsonr(y_test, y_pred)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_ts = end_time - start_time\n",
    "\n",
    "    score_ts = pd.DataFrame({'r2':r2_ts, 'rvalue':r_value_ts, 'time':time_ts}, index=[0])\n",
    "    score_ts.to_csv(f\"{file_path}{file_name}_ts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigkernel_score(X_train,X_test,y_train,y_test,method,file_path,dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    This function is used to calculate the score of the sigkernel method and will save the score in a csv file.\n",
    "\n",
    "    Parameters:\n",
    "            X_train: is the training data with shape (num_samples, num_bins(time), num_features)\n",
    "            X_test: is the testing data with shape (num_samples, num_bins(time), num_features)\n",
    "            y_train: is the training label with shape (num_samples,)\n",
    "            y_test: is the testing label with shape (num_samples,)\n",
    "            method: is the name of the method to be used, followings are the list of methods that can be use\n",
    "            file_path: is the path to save the score\n",
    "            dataset: is the name of the dataset\n",
    "\n",
    "    Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    file_name = f\"{dataset}\"\n",
    "    _sigmas = [1e-2, 2.5e-2, 5e-2, 7.5e-2, 1e-1, 1.5e-1, \n",
    "               2e-1, 2.5e-1, 3e-1, 3.5e-1, 4e-1, 4.5e-1, \n",
    "               5e-1, 5.5e-1, 6e-1, 6.5e-1, 7e-1, 7.5e-1, \n",
    "               8e-1, 8.5e-1, 9e-1, 9.5e-1, 1.]\n",
    "    start_time = time.time()\n",
    "\n",
    "    if X_train.shape[0] <= 150 and X_train.shape[1] <=150 and X_train.shape[2] <= 8:\n",
    "                    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                    dtype = torch.float32\n",
    "    else: # otherwise do computations in cython\n",
    "        device = 'cpu'\n",
    "        dtype = torch.float64\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=dtype, device=device)\n",
    "    X_test = torch.tensor(X_test, dtype=dtype, device=device)\n",
    "\n",
    "    best_scores = 0\n",
    "    best_sigma = None\n",
    "    \n",
    "    for sigma in tqdm(_sigmas):\n",
    "\n",
    "            # define static kernel\n",
    "            static_kernel = sigkernel.RBFKernel(sigma=sigma)\n",
    "\n",
    "            # initialize corresponding signature PDE kernel\n",
    "            signature_kernel = sigkernel.SigKernel(static_kernel, dyadic_order=0)\n",
    "\n",
    "            # compute Gram matrix on train data\n",
    "            G_train = signature_kernel.compute_Gram(X_train, X_train, sym=True).cpu().numpy()\n",
    "\n",
    "            if method.startswith('r_'):\n",
    "                clf = ml_method_setup('r_svr', G_train, y_train, False)\n",
    "            else:\n",
    "                clf = ml_method_setup('svc', G_train, y_train, False)\n",
    "\n",
    "            del G_train\n",
    "\n",
    "            if clf.best_score_ > best_scores:\n",
    "                best_scores = clf.best_score_\n",
    "                best_sigma = sigma\n",
    "            \n",
    "    # define static kernel\n",
    "    static_kernel = sigkernel.RBFKernel(sigma = best_sigma)\n",
    "    # initialize corresponding signature PDE kernel\n",
    "    signature_kernel = sigkernel.SigKernel(static_kernel, dyadic_order=0)\n",
    "    # compute Gram matrix on test data\n",
    "    G_test = signature_kernel.compute_Gram(X_test, X_train, sym=False).cpu().numpy()\n",
    "\n",
    "    y_pred = clf.predict(G_test)\n",
    "    r2_sigkernel = r2_score(y_test, y_pred)\n",
    "    r_value_sigkernel, p_value_sigkernel = pearsonr(y_test, y_pred)\n",
    "    end_time = time.time()\n",
    "    time_sigkernel = end_time - start_time\n",
    "\n",
    "    score_sigkernel = pd.DataFrame({'r2':r2_sigkernel, 'rvalue':r_value_sigkernel, 'time':time_sigkernel}, index=[0])\n",
    "    score_sigkernel.to_csv(f\"{file_path}{file_name}_sigkernel.csv\", index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto(X_train, X_test, y_train, y_test, \n",
    "             dataset, method, \n",
    "             reduced, bin_reduce = True,\n",
    "             file_path = '.'):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function is used to run the auto pipeline.\n",
    "\n",
    "    Parameters:\n",
    "            X_train: is the training data. \n",
    "                     For classification, it will have the shape (num_samples, num_timepoints, num_features). \n",
    "                     For regression, it will have the shape (num_timepoints, num_features)\n",
    "\n",
    "            X_test: is the testing data.\n",
    "                        For classification, it will have the shape (num_samples, num_timepoints, num_features).\n",
    "                        For regression, it will have the shape (num_timepoints, num_features)\n",
    "\n",
    "            y_train: is the training label.\n",
    "                        For classification, it will have the shape (num_samples,).\n",
    "                        For regression, it will have the shape (num_timepoints,)\n",
    "\n",
    "            y_test: is the testing label.\n",
    "                        For classification, it will have the shape (num_samples,).\n",
    "                        For regression, it will have the shape (num_timepoints,)\n",
    "\n",
    "            dataset: is the name of the dataset\n",
    "            method: is the name of the method to be used, followings are the list of methods that can be use\n",
    "            reduced: is a boolean variable, if True, the hyperparameters will be reduced to save time\n",
    "            bin_reduce: is a boolean variable, if True, the bins_before hyperparameters will be reduced to save time\n",
    "            file_path: is the path to save the score\n",
    "\n",
    "    Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = f\"{dataset}_{method}\"\n",
    "   \n",
    "\n",
    "    if method.startswith('r_'):\n",
    "        \n",
    "        # ============ grid search for bins ============\n",
    "        bin_before = grid_search_bins(X_train,y_train,X_test,y_test,method,bin_reduce,reduced)\n",
    "        X_train,y_train,X_test,y_test = data_bins(X_train,y_train,X_test,y_test,bin_before,0,1)\n",
    "        \n",
    "\n",
    "        # ========= grid search for ll & at & scale ================\n",
    "        best_at, best_ll, best_scaler = grid_search_atll(X_train,X_test,y_train,y_test,reduced)\n",
    "        X_train, X_test = data_process(X_train, X_test, best_at, best_ll, best_scaler)\n",
    "\n",
    "        # ========= subsample ================\n",
    "        X_train, X_test = subsample(X_train,X_test)\n",
    "\n",
    "        if best_scaler == 'minmax':\n",
    "            file_name += \"_minmax\"\n",
    "        elif best_scaler == 'standard':\n",
    "            file_name += \"_standard\"\n",
    "        else:\n",
    "            file_name += \"_none\"\n",
    "\n",
    "        if best_at:\n",
    "            file_name += \"_at\"\n",
    "        if best_ll:\n",
    "            file_name += \"_ll\"\n",
    "    \n",
    "\n",
    "        if method.startswith('r_ts'):\n",
    "            ts_score(X_train,X_test,y_train,y_test,method,file_path,file_name,reduced = False)\n",
    "\n",
    "        else:\n",
    "            # ==== signature ====\n",
    "            signature_score(X_train,X_test,y_train,y_test,method,file_path,file_name,reduced = False)\n",
    "            # ==== flatten =====\n",
    "            flatten_score(X_train,X_test,y_train,y_test,method,file_path,file_name,reduced = False)\n",
    "            # ==== sigkernel ====\n",
    "            sigkernel_score(X_train,X_test,y_train,y_test,method,file_path,dataset)\n",
    "\n",
    "    else:\n",
    "        # ========= grid search for ll & at & scale ================\n",
    "        best_at, best_ll, best_scaler = grid_search_atll(X_train,X_test,y_train,y_test,reduced)\n",
    "        X_train, X_test = data_process(X_train, X_test, best_at, best_ll, best_scaler)\n",
    "        X_train, X_test = subsample(X_train,X_test)\n",
    "\n",
    "        if best_scaler == 'minmax':\n",
    "            file_name += \"_minmax\"\n",
    "        elif best_scaler == 'standard':\n",
    "            file_name += \"_standard\"\n",
    "        else:\n",
    "            file_name += \"_none\"\n",
    "\n",
    "        if best_at:\n",
    "            file_name += \"_at\"\n",
    "        if best_ll:\n",
    "            file_name += \"_ll\"\n",
    "        \n",
    "        if method.startswith('ts'):\n",
    "            ts_score(X_train,X_test,y_train,y_test,method,file_path,file_name,reduced = False)\n",
    "\n",
    "\n",
    "        else:\n",
    "            # ====== signature ======\n",
    "            signature_score(X_train,X_test,y_train,y_test,method,file_path,file_name,reduced = False)\n",
    "\n",
    "            # ====== flatten ======\n",
    "            flatten_score(X_train,X_test,y_train,y_test,method,file_path,file_name,reduced = False)\n",
    "\n",
    "            # ====== sigkernel ======\n",
    "            sigkernel_score(X_train,X_test,y_train,y_test,method,file_path,dataset)\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
