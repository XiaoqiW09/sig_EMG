{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: statsmodels is not installed. You will be unable to use the Naive Bayes Decoder\n",
      "\n",
      "WARNING: Xgboost package is not installed. You will be unable to use the xgboost decoder\n",
      "\n",
      "WARNING: Keras package is not installed. You will be unable to use all neural net decoders\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import iisignature as sig\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,Normalizer,MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score,r2_score,mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,RandomForestRegressor,GradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegression,Lasso\n",
    "from sklearn.svm import SVC,SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from tslearn.svm import TimeSeriesSVC,TimeSeriesSVR\n",
    "from tslearn.neighbors import KNeighborsTimeSeriesClassifier\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from tslearn.neural_network import TimeSeriesMLPRegressor\n",
    "\n",
    "from Neural_Decoding.preprocessing_funcs import get_spikes_with_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_method_setup(method, X_train, y_train):\n",
    "    \n",
    "        if method == 'ts_knn':\n",
    "                parameters = {'n_neighbors': [1, 3, 5, 7],'metric': ['euclidean', 'dtw']}\n",
    "                clf = GridSearchCV(KNeighborsTimeSeriesClassifier(),\n",
    "                                parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'ts_svc':\n",
    "                parameters = {'C': [0.1, 1, 10],'kernel': ['linear', 'rbf'],'gamma': ['scale', 'auto', 0.1]}\n",
    "                clf = GridSearchCV(TimeSeriesSVC(random_state=0, probability=True),\n",
    "                                parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "        \n",
    "        elif method == 'r_ts_svr':\n",
    "                parameters = {'C': [0.1, 1, 10],'epsilon': [0.01, 0.1, 0.5],  'kernel': ['linear', 'rbf']}\n",
    "                clf = GridSearchCV(TimeSeriesSVR(),\n",
    "                                parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'r_ts_neuralnetwork':\n",
    "                parameters = {'hidden_layer_sizes': [(50,), (100,), (50, 50)], \n",
    "                           'activation': ['relu', 'tanh'],  \n",
    "                           'learning_rate': ['constant', 'adaptive'],  \n",
    "                           'alpha': [0.0001, 0.001, 0.01]}\n",
    "                clf = GridSearchCV(TimeSeriesMLPRegressor(),\n",
    "                                parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'logisticregression':\n",
    "\n",
    "                lr = LogisticRegression(random_state=0)\n",
    "                parameters = {'C': [0.1, 0.2, 0.5, 1, 2, 5, 10],\n",
    "                                'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "                clf = GridSearchCV(lr, parameters,cv =5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'svc':\n",
    "                svc = SVC(random_state=0, probability=True)\n",
    "                parameters = {'kernel': ['rbf', 'poly'], 'shrinking': [True, False],\n",
    "                                'C': [0.1, 1, 10]}\n",
    "                clf = GridSearchCV(svc, parameters,cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'knnclassifier':\n",
    "                knn = KNeighborsClassifier()\n",
    "                parameters = {'n_neighbors': range(3, 30, 2), 'weights': ['uniform', 'distance']}\n",
    "                clf = GridSearchCV(knn, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'adaboostclassifier':\n",
    "                ada = AdaBoostClassifier(random_state=0)\n",
    "                parameters = {'n_estimators': [50, 100], 'learning_rate': [0.5, 1, 2]}\n",
    "                clf = GridSearchCV(ada, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'randomforestclassifier':\n",
    "            rf = RandomForestClassifier(random_state=0)\n",
    "            parameters = {'min_weight_fraction_leaf': [0.1, 0.5],\n",
    "                            'bootstrap': [True, False],\n",
    "                            'max_depth': (2, 5),\n",
    "                            'max_leaf_nodes': (2, 5),\n",
    "                            'n_estimators': (100, 200)}\n",
    "            clf = GridSearchCV(rf, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'r_lassoregression':\n",
    "                lr = Lasso()\n",
    "                parameters = {'alpha': [0.1, 0.2, 0.5, 1, 2, 5, 10]}\n",
    "                clf = GridSearchCV(lr, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "        \n",
    "        elif method == 'r_svr':\n",
    "                svr = SVR()\n",
    "                parameters = {'C': [0.1, 1.0, 10.0], 'kernel': ['linear', 'rbf'], \n",
    "                               'gamma': ['scale', 'auto']}\n",
    "                clf = GridSearchCV(svr, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'r_randomforestregression':\n",
    "                rf = RandomForestRegressor()\n",
    "                parameters =  {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20], 'min_samples_split': [2, 5, 10]}\n",
    "                clf = GridSearchCV(rf, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'r_gradientboostingregression':\n",
    "                gb = GradientBoostingRegressor()\n",
    "                parameters= {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}\n",
    "                clf = GridSearchCV(gb, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "        elif method == 'r_neuralnetwork':\n",
    "                nn = MLPRegressor()\n",
    "                parameters = {'hidden_layer_sizes': [(50,), (100,), (50, 50)], 'activation': ['relu', 'tanh'], 'alpha': [0.0001, 0.001, 0.01]}\n",
    "                clf = GridSearchCV(nn, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "                clf.fit(X_train, y_train)\n",
    "        \n",
    "        elif method == 'r_GaussianNB':\n",
    "                kernel = 1.0 * RBF(length_scale=1.0)\n",
    "                model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=42)\n",
    "                parameters = {'kernel__length_scale': [0.1, 1.0, 10.0],}        \n",
    "                clf = GridSearchCV(model, parameters, cv=5, n_jobs=-1,verbose=10,scoring='neg_mean_squared_error')\n",
    "                clf.fit(X_train, y_train)\n",
    "        else:\n",
    "                clf = None\n",
    "\n",
    "        return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_augment(arr):\n",
    "    return np.vstack((arr.T, np.linspace(0, 1, num=arr.shape[0]))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scale_process(X_train,X_test):\n",
    "    # data: np.array (num_samples, num_timestamps, num_channels)\n",
    "    data_mean = np.nanmean(X_train,axis=0)\n",
    "    data_std = np.nanstd(X_train,axis=0)\n",
    "    if 0 in data_std:\n",
    "      print('error')\n",
    "    X_train = (X_train-data_mean)/data_std\n",
    "    X_test = (X_test-data_mean)/data_std\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_bins(X_train,y_train,X_test,y_test,bins_before,bins_after,bins_current):\n",
    "\n",
    "    # use bin decoding to create array with (num_samples, num_bins(time), num_features)\n",
    "  if bins_before == 0:\n",
    "    X_train =get_spikes_with_history(X_train,bins_before,bins_after,bins_current)[:-bins_after,:,:]\n",
    "    X_test = get_spikes_with_history(X_test,bins_before,bins_after,bins_current)[:-bins_after,:,:]\n",
    "    y_train = y_train[:-bins_after]\n",
    "    y_test = y_test[:-bins_after]\n",
    "\n",
    "  elif bins_after == 0:\n",
    "    X_train =get_spikes_with_history(X_train,bins_before,bins_after,bins_current)[bins_before:,:,:]\n",
    "    X_test = get_spikes_with_history(X_test,bins_before,bins_after,bins_current)[bins_before:,:,:]\n",
    "    y_train = y_train[bins_before:]\n",
    "    y_test = y_test[bins_before:]\n",
    "\n",
    "  else:\n",
    "    X_train =get_spikes_with_history(X_train,bins_before,bins_after,bins_current)[bins_before:-bins_after,:,:]\n",
    "    X_test = get_spikes_with_history(X_test,bins_before,bins_after,bins_current)[bins_before:-bins_after,:,:]\n",
    "    y_train = y_train[bins_before:-bins_after]\n",
    "    y_test = y_test[bins_before:-bins_after]\n",
    "\n",
    "  return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_flatten(X_train, X_test):\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(X_train, X_test, scale='minmax' , time_aug=False):\n",
    "    \n",
    "    # X_train shape: (n_samples, n_features, n_timestamps)\n",
    "    # X_test shape: (n_samples, n_features, n_timestamps)\n",
    "\n",
    "    # ============ Data Preprocessing ============ \n",
    "    ts_min_max_scaler = TimeSeriesScalerMinMax()\n",
    "\n",
    "    # ============ minmax =============\n",
    "    if scale == 'minmax':\n",
    "        scaler = ts_min_max_scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    # ============ standard =============\n",
    "    elif scale == 'standard':\n",
    "        print(X_train.shape)\n",
    "        X_train, X_test = standard_scale_process(X_train, X_test)\n",
    "    \n",
    "    else: \n",
    "        pass\n",
    "\n",
    "    # ============ time aurment =============\n",
    "    if time_aug:\n",
    "        X_train = np.array(list(map(time_augment, X_train)))\n",
    "        X_test = np.array(list(map(time_augment, X_test)))\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signature(X_train,X_test,sig_level=2):\n",
    "    # hape of X_train: (n_samples,n_timestamps,n_features)\n",
    "    X_train = sig.sig(X_train,sig_level)\n",
    "    X_test = sig.sig(X_test,sig_level)\n",
    "\n",
    "    return X_train,X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_bins(X_train,y_train,X_test,y_test,reduced=True):\n",
    "    if reduced:\n",
    "        bins_before = [10,50,100]\n",
    "        bins_after  = [0,10]\n",
    "        bins_current = [0,1]\n",
    "    else:\n",
    "        bins_before = [5,10,25,50,75,100,250,500]\n",
    "        bins_after = [5,10,25,50,75,100,250,500]\n",
    "        bins_current = [0,1]\n",
    "\n",
    "    r2_best = 0\n",
    "    bin_b_best = 0\n",
    "    bin_a_best = 0\n",
    "    bin_c_best = 0\n",
    "\n",
    "    for bin_before in tqdm(bins_before):\n",
    "        for bin_after in bins_after:\n",
    "            for bin_current in bins_current:\n",
    "                X_train_b,y_train_b,X_test_b,y_test_b = data_bins(X_train,y_train,X_test,y_test,bin_before,bin_after,bin_current)\n",
    "                X_train_flat, X_test_flat = data_flatten(X_train_b, X_test_b)\n",
    "                kf = KFold(n_splits=3)\n",
    "                \n",
    "                for train_index, test_index in kf.split(X_train_flat):\n",
    "                    X_train_cv, X_test_cv = X_train_flat[train_index], X_train_flat[test_index]\n",
    "                    y_train_cv, y_test_cv = y_train_b[train_index], y_train_b[test_index]\n",
    "                    clf = SVR()\n",
    "                    clf.fit(X_train_cv,y_train_cv)\n",
    "                    y_pred = clf.predict(X_test_cv)\n",
    "                    r2 = r2_score(y_test_cv, y_pred)\n",
    "                    if r2 > r2_best:\n",
    "                        bin_b_best = bin_before\n",
    "                        bin_a_best = bin_after\n",
    "                        bin_c_best = bin_current\n",
    "                        r2_best = r2\n",
    "\n",
    "    return bin_b_best,bin_a_best,bin_c_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto(X_train, X_test, y_train, y_test, \n",
    "             dataset, method, sig_level, \n",
    "             scale, time_aug,\n",
    "             bins_before, bins_after, bins_current,\n",
    "             file_path = '.'\n",
    "             ):\n",
    "    \n",
    "    \n",
    "\n",
    "    file_name = f\"{dataset}_{method}\"\n",
    "    if scale == 'minmax':\n",
    "        file_name += \"_minmax\"\n",
    "    elif scale == 'standard':\n",
    "        file_name += \"_standard\"\n",
    "    else:\n",
    "        file_name += \"_none\"\n",
    "\n",
    "    if time_aug:\n",
    "        file_name += \"_timeaug\"\n",
    "\n",
    "    if method.startswith('r_'):\n",
    "        \n",
    "        \n",
    "        X_train,y_train,X_test,y_test = data_bins(X_train,y_train,X_test,y_test,bins_before,bins_after,bins_current)\n",
    "        X_train,X_test = data_process(X_train, X_test, scale, time_aug)\n",
    "\n",
    "        if method.startswith('r_ts'):\n",
    "            start_time = time.time()\n",
    "            clf = ml_method_setup(method, X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            r2_ts = r2_score(y_test, y_pred)\n",
    "            r_value_ts, p_value_ts = pearsonr(y_test, y_pred)\n",
    "            end_time = time.time()\n",
    "            time_ts = end_time - start_time\n",
    "\n",
    "            score_ts = pd.DataFrame({'r2':r2_ts, 'rvalue':r_value_ts, 'time':time_ts}, index=[0])\n",
    "            score_ts.to_csv(f\"{file_path}{file_name}_ts.csv\", index=False)\n",
    "\n",
    "        else:\n",
    "            # ==== signature ====\n",
    "            start_time = time.time()\n",
    "            X_train_sig, X_test_sig = signature(X_train, X_test, sig_level)\n",
    "            clf_sig = ml_method_setup(method, X_train_sig, y_train)\n",
    "            y_pred_sig = clf_sig.predict(X_test_sig)\n",
    "            r2_sig = r2_score(y_test, y_pred_sig)\n",
    "            r_value_sig, p_value_sig = pearsonr(y_test, y_pred_sig)\n",
    "            end_time = time.time()\n",
    "            time_sig = end_time - start_time\n",
    "\n",
    "            score_sig = pd.DataFrame({'r2':r2_sig, 'rvalue':r_value_sig, 'time':time_sig}, index=[0])\n",
    "            score_sig.to_csv(f\"{file_path}{file_name}_sig.csv\", index=False)\n",
    "            \n",
    "            \n",
    "\n",
    "            # ==== flatten =====\n",
    "            start_time = time.time()\n",
    "            X_train_flat, X_test_flat = data_flatten(X_train, X_test)\n",
    "            clf_flat = ml_method_setup(method, X_train_flat, y_train)\n",
    "            y_pred_flat = clf_flat.predict(X_test_flat)\n",
    "            r2_flat = r2_score(y_test, y_pred_flat)\n",
    "            r_value_flat, p_value_flat = pearsonr(y_test, y_pred_flat)\n",
    "            end_time = time.time()\n",
    "            time_flat = end_time - start_time\n",
    "\n",
    "            score_flat = pd.DataFrame({'r2':r2_flat, 'rvalue':r_value_flat, 'time':time_flat}, index=[0])\n",
    "            score_flat.to_csv(f\"{file_path}{file_name}_flat.csv\", index=False)\n",
    "\n",
    "    else:\n",
    "        X_train, X_test = data_process(X_train, X_test, scale, time_aug)\n",
    "        print(X_train.shape)\n",
    "        \n",
    "        if method.startswith('ts'):\n",
    "            start_time = time.time()\n",
    "            clf = ml_method_setup(method, X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            end_time = time.time()\n",
    "            time_ts = end_time - start_time\n",
    "\n",
    "            score_ts = pd.DataFrame({'accuracy_score':accuracy, 'time':time_ts}, index=[0])\n",
    "            score_ts.to_csv(f\"{file_path}{file_name}_ts.csv\", index=False)\n",
    "\n",
    "        else:\n",
    "            # ====== signature ======\n",
    "            start_time = time.time()\n",
    "            X_train_sig, X_test_sig = signature(X_train, X_test, sig_level)\n",
    "            clf_sig = ml_method_setup(method, X_train_sig, y_train)\n",
    "            y_pred_sig = clf_sig.predict(X_test_sig)\n",
    "            accuracy_score_sig = accuracy_score(y_test, y_pred_sig)\n",
    "            end_time = time.time()\n",
    "            time_sig = end_time - start_time\n",
    "\n",
    "            score_sig = pd.DataFrame({'accuracy_score':accuracy_score_sig, 'time':time_sig}, index=[0])\n",
    "            score_sig.to_csv(f\"{file_path}{file_name}_sig.csv\", index=False)\n",
    "\n",
    "            # ====== flatten ======\n",
    "            start_time = time.time()\n",
    "            X_train_flat, X_test_flat = data_flatten(X_train, X_test)\n",
    "            clf_flat = ml_method_setup(method, X_train_flat, y_train)\n",
    "            y_pred_flat = clf_flat.predict(X_test_flat)\n",
    "            accuracy_score_flat = accuracy_score(y_test, y_pred_flat)\n",
    "            end_time = time.time()\n",
    "            time_flat = end_time - start_time\n",
    "            \n",
    "            score_flat = pd.DataFrame({'accuracy_score':accuracy_score_flat, 'time':time_flat}, index=[0])\n",
    "            score_flat.to_csv(f\"{file_path}{file_name}_flat.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
